Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Tufte2008,
abstract = {Biological organisms have an inherent ability to respond to environmental changes. The response can emerge as organ- isms that can develop into structural and behavioural differ- ent phenotypes. To achieve such properties in an artificial developmental setting external environmental information is included in the gene regulation of the developmental model. This implies interplay between evolution, development and the environment. An experimental approach is taken to in- vestigate this interplay. The test case chosen is evolution of robustness to environmental fluctuations. Development mod- els with and without environmental information included in the gene regulation are compared. Further, the developing organisms of the two models are exposed to environmental fluctuations for a more extensive investigation. The results indicate that including external information in the gene regu- lation can be favourable and exploitable, particularly for or- ganisms developing in a dynamic environment.},
author = {Tufte, Gunnar},
booktitle = {Artificial life XI The Eleventh International Conference on the Simulation and Synthesis of Living Systems},
isbn = {9780262750172},
pages = {624--631},
title = {{Evolution, Development and Environment Toward Adaptation through Phenotypic Plasticity and Exploitation of External Information}},
url = {http://mitpress.mit.edu/books/chapters/0262287196chap81.pdf},
volume = {11},
year = {2008}
}
@article{Lawrence2001,
abstract = {Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarily, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. We introduce a new intelligent signal processing method which addresses the difficulties. The method proposed uses conversion into a symbolic representation with a self-organizing map, and grammatical inference with recurrent neural networks. We apply the method to the prediction of daily foreign exchange rates, addressing difficulties with non-stationarily, overfitting, and unequal a priori class probabilities, and we find significant predictability in comprehensive experiments covering 5 different foreign exchange rates. The method correctly predicts the direction of change for the next day with an error rate of 47.1{\%}. The error rate reduces to around 40{\%} when rejecting examples where the system has low confidence in its prediction. We show that the symbolic representation aids the extraction of symbolic knowledge from the trained recurrent neural networks in the form of deterministic finite state automata. These automata explain the operation of the system and are often relatively simple. Automata rules related to well known behavior such as trend following and mean reversal are extracted.},
author = {Lawrence, Steve and Giles, CL Lee and Fong, S and Lawrence, Steve and Tsoi, Ah Chung AC},
doi = {10.1023/A:1010884214864},
issn = {0885-6125},
journal = {Machine Learning},
number = {1},
pages = {161--183},
title = {{Noisy Time Series Prediction using Recurrent Neural Networks and Grammatical Inference}},
url = {http://link.springer.com/article/10.1023/A:1010884214864{\%}5Cnhttp://www.springerlink.com/index/X15G111873157W14.pdf{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=842255},
volume = {44},
year = {2001}
}
@misc{Bohte2002,
abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Bohte, Sander M. and Kok, Joost N. and {La Poutr{\'{e}}}, Han},
booktitle = {Neurocomputing},
doi = {10.1016/S0925-2312(01)00658-0},
isbn = {0925-2312},
issn = {09252312},
keywords = {Error-backpropagation,Spiking neurons,Temporal coding},
pages = {17--37},
pmid = {1000253838},
title = {{Error-backpropagation in temporally encoded networks of spiking neurons}},
volume = {48},
year = {2002}
}
@inproceedings{Haddow2000a,
abstract = {Can we realise the opportunities that lie in design by evolution$\backslash$nby using traditional technologies or are there better technologies which$\backslash$nwill allow us to fully realise the potential inherent in evolvable$\backslash$nhardware? The authors consider the characteristics of evolvable$\backslash$nhardware, especially for adaptive design, and discuss the demands that$\backslash$nthese characteristics place on the underlying technology. They suggest a$\backslash$npotential alternative to today's FPGA technology. The proposed$\backslash$narchitecture is particularly focused at reducing the genotype required$\backslash$nfor a given design by reducing the configuration data required for$\backslash$nunused routing resources and allowing partial configuration down to a$\backslash$nsingle CLB. In addition, to support adaptive hardware,$\backslash$nself-reconfiguration is enabled},
author = {Haddow, Pauline C. and Tufte, Gunnar},
booktitle = {Proceedings of the 2000 Congress on Evolutionary Computation, CEC 2000},
doi = {10.1109/CEC.2000.870345},
isbn = {0-7803-6375-2},
pages = {553--560},
title = {{An evolvable hardware FPGA for adaptive hardware}},
volume = {1},
year = {2000}
}
@article{Mitchell1993,
abstract = {We present results from an experiment similar to one performed by Packard (1988), in which a genetic algorithm is used to evolve cellular automata (CA) to perform a particular computational task. Packard examined the frequency of evolved CA rules as a function of Langton's lambda parameter (Langton, 1990), and interpreted the results of his experiment as giving evidence for the following two hypotheses: (1) CA rules able to perform complex computations are most likely to be found near ``critical'' lambda values, which have been claimed to correlate with a phase transition between ordered and chaotic behavioral regimes for CA; (2) When CA rules are evolved to perform a complex computation, evolution will tend to select rules with lambda values close to the critical values. Our experiment produced very different results, and we suggest that the interpretation of the original results is not correct. We also review and discuss issues related to lambda, dynamical-behavior classes, and computation in CA. The main constructive results of our study are identifying the emergence and competition of computational strategies and analyzing the central role of symmetries in an evolutionary system. In particular, we demonstrate how symmetry breaking can impede the evolution toward higher computational capability.},
archivePrefix = {arXiv},
arxivId = {adap-org/9303003},
author = {Mitchell, Melanie and Hraber, Peter and Crutchfield, James P.},
eprint = {9303003},
journal = {arXiv preprint adap-org/9303003},
pages = {38},
primaryClass = {adap-org},
title = {{Revisiting the Edge of Chaos: Evolving Cellular Automata to Perform Computations}},
url = {http://arxiv.org/abs/adap-org/9303003},
volume = {7},
year = {1993}
}
@article{Bianconi2013,
abstract = {AbstractBackground: All living organisms are made of individual and identifiable cells, whose number, together with their size and type, ultimately defines the structure and functions of an organism. While the total cell number of lower organisms is often known, it has not yet been defined in higher organisms. In particular, the reported total cell number of a human being ranges between 1012 and 1016 and it is widely mentioned without a proper reference.Aim: To study and discuss the theoretical issue of the total number of cells that compose the standard human adult organism.Subjects and methods: A systematic calculation of the total cell number of the whole human body and of the single organs was carried out using bibliographical and/or mathematical approaches.Results: A current estimation of human total cell number calculated for a variety of organs and cell types is presented. These partial data correspond to a total number of 3.72 × 1013.Conclusions: Knowing the total cell number of the human body as ...},
author = {Bianconi, Eva and Piovesan, Allison and Facchin, Federica and Et al},
journal = {Annals of Human Biology},
title = {{An estimation of the number of cells in the human body}},
year = {2013}
}
@article{Fernando2003,
abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a {\&}8220;Liquid State Machine{\&}8221; that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass{\&}8217; Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this {\&}8220;for free{\&}8221;, and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
author = {Fernando, Chrisantha and Sojakka, Sampsa},
doi = {10.1007/978-3-540-39432-7_63},
isbn = {9783540200574},
issn = {03029743},
journal = {Advances in Artificial Life},
pages = {588--597},
title = {{Pattern Recognition in a Bucket}},
url = {http://www.springerlink.com/content/xlnymhf0qp946rce},
year = {2003}
}
@article{KosakovskyPond2006,
abstract = {MOTIVATION: Phylogenetic and evolutionary inference can be severely misled if recombination is not accounted for, hence screening for it should be an essential component of nearly every comparative study. The evolution of recombinant sequences can not be properly explained by a single phylogenetic tree, but several phylogenies may be used to correctly model the evolution of non-recombinant fragments. RESULTS: We developed a likelihood-based model selection procedure that uses a genetic algorithm to search multiple sequence alignments for evidence of recombination breakpoints and identify putative recombinant sequences. GARD is an extensible and intuitive method that can be run efficiently in parallel. Extensive simulation studies show that the method nearly always outperforms other available tools, both in terms of power and accuracy and that the use of GARD to screen sequences for recombination ensures good statistical properties for methods aimed at detecting positive selection. AVAILABILITY: Freely available http://www.datamonkey.org/GARD/},
author = {{Kosakovsky Pond}, Sergei L. and Posada, David and Gravenor, Michael B. and Woelk, Christopher H. and Frost, Simon D W},
doi = {10.1093/bioinformatics/btl474},
isbn = {1460-2059 (Electronic)},
issn = {13674803},
journal = {Bioinformatics},
number = {24},
pages = {3096--3098},
pmid = {17110367},
title = {{GARD: A genetic algorithm for recombination detection}},
volume = {22},
year = {2006}
}
@inproceedings{Sanchez1997,
abstract = {Living beings are complex systems exhibiting a range of desirable qualifications that have eluded realization by traditional engineering methodologies. In recent years we are witness to a growing interest in Nature exhibited by engineers, wishing to imitate the observed processes, thereby creating powerful problem-solving methodologies. If one considers Life on earth since its very beginning, three levels of organization can be distinguished: the phylogenetic level concerns the temporal evolution of the genetic programs within individuals and species, the ontogenetic level concerns the developmental process of a single multicellular organism, and the epigenetic level concerns the learning processes during an individual organism's lifetime. In analogy to Nature, the space of bioinspired systems can be partitioned along these three axes, phylogeny, ontogeny, and epigenesis, giving rise to the POE model. This paper is an exposition and examination of bio-inspired systems within the POE framework. We first discuss each of the three axes separately, considering the systems created to date and plotting directions for continued progress along the axis in question. We end our exposition by a discussion of possible research directions, involving the construction of bio-inspired systems that are situated along two, and ultimately all three axes. This presents a vision for the future which will see the advent of novel systems, inspired by the powerful examples provided by Nature.},
author = {Sanchez, Eduardo and Mange, Daniel and Sipper, Moshe and Tomassini, Marco and Perez-Uribe, Andres and Stauffer, Andr??},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/3-540-63173-9_37},
isbn = {3540631739},
issn = {16113349},
pages = {33--54},
title = {{Phylogeny, ontogeny, and epigenesis: Three sources of biological inspiration for softening hardware}},
volume = {1259},
year = {1997}
}
@techreport{Saban2011,
author = {Saban, Kirk},
file = {:Users/oyvinrob/Downloads/wp380{\_}Stacked{\_}Silicon{\_}Interconnect{\_}Technology.pdf:pdf},
keywords = {SSI,Virtex-7,stacked silicon interconnect},
title = {{Interconnect Technology Delivers Breakthrough FPGA Capacity , Bandwidth , and Power Efficiency}},
year = {2012}
}
@inproceedings{Fitzgerald:2015:IAS:2739480.2754761,
address = {New York, NY, USA},
author = {Fitzgerald, Jeannie M and Ryan, Conor and Medernach, David and Krawiec, Krzysztof},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
doi = {10.1145/2739480.2754761},
isbn = {978-1-4503-3472-3},
keywords = { mammography, multi-objective genetic programming,classification},
pages = {1199--1206},
publisher = {ACM},
series = {GECCO '15},
title = {{An Integrated Approach to Stage 1 Breast Cancer Detection}},
url = {http://doi.acm.org/10.1145/2739480.2754761},
year = {2015}
}
@article{Maass1997,
abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
author = {Maass, Wolfgang},
doi = {10.1016/S0893-6080(97)00011-7},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
number = {9},
pages = {1659--1671},
title = {{Networks of spiking neurons: The third generation of neural network models}},
volume = {10},
year = {1997}
}
@article{Wolfram1984,
abstract = {Cellular automata are discrete dynamical systems with simple construction but complex self-organizing behaviour. Evidence is presented that all one-dimensional cellular automata fall into four distinct universality classes. Characterizations of the structures generated in these classes are discussed. Three classes exhibit behaviour analogous to limit points, limit cycles and chaotic attractors. The fourth class is probably capable of universal computation, so that properties of its infinite time behaviour are undecidable. ?? 1984.},
author = {Wolfram, Stephen},
doi = {10.1016/0167-2789(84)90245-8},
isbn = {0167-2789},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-2},
pages = {1--35},
title = {{Universality and complexity in cellular automata}},
volume = {10},
year = {1984}
}
@inproceedings{Steil2004,
abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08836v1},
author = {Steil, Jochen J.},
booktitle = {IEEE International Conference on Neural Networks - Conference Proceedings},
doi = {10.1109/IJCNN.2004.1380039},
eprint = {arXiv:1506.08836v1},
isbn = {0780383591},
issn = {10987576},
pages = {843--848},
title = {{Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity}},
volume = {2},
year = {2004}
}
@article{Maass2002,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {11},
pages = {2531--2560},
pmid = {12433288},
title = {{Real-time computing without stable states: a new framework for neural computation based on perturbations.}},
volume = {14},
year = {2002}
}
@article{Tufte2016,
author = {Tufte, Gunnar and Lykkeb{\o}, Odd Rune},
file = {:Users/oyvinrob/Library/Application Support/Mendeley Desktop/Downloaded/Tufte, Lykkeb{\o} - 2016 - Evolution-in-Materio of a dynamical system with dynamical structures.pdf:pdf},
isbn = {9780262339360},
journal = {Proceedings of the Artificial Life Conference 2016},
title = {{Evolution-in-Materio of a dynamical system with dynamical structures}},
year = {2016}
}
@article{Langton1990,
abstract = {In order for computation to emerge spontaneously and become an important factor in the dynamics of a system, the material substrate must support the primitive functions required for computation: the transmission, storage, and modification of information. Under what conditions might we expect physical systems to support such computational primitives? This paper presents research on cellular automata which suggests that the optimal conditions for the support of information transmission, storage, and modification, are achieved in the vicinity of a phase transition. We observe surprising similarities between the behaviors of computations and systems near phase transitions, finding analogs of computational complexity classes and the halting problem within the phenomenology of phase transitions. We conclude that there is a fundamental connection between computation and phase transitions, especially second-order or "critical" transitions, and discuss some of the implications for our understanding of nature if such a connection is borne out. ?? 1990.},
archivePrefix = {arXiv},
arxivId = {adap-org/9306003},
author = {Langton, Chris G.},
doi = {10.1016/0167-2789(90)90064-V},
eprint = {9306003},
isbn = {0-7382-0232-0},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {12--37},
pmid = {3751941},
primaryClass = {adap-org},
title = {{Computation at the edge of chaos: Phase transitions and emergent computation}},
volume = {42},
year = {1990}
}
@article{Sipper2004,
author = {Sipper, Moshe},
file = {:Users/oyvinrob/Downloads/sipper-evolveca.pdf:pdf},
isbn = {3540626131},
title = {{Evolution of Parallel Cellular Machines: The Cellular Programming Approach}},
volume = {2004},
year = {2004}
}
@book{Hall2003,
author = {Hall, Brian K. and Pearson, Roy D. and M{\"{u}}ller, Gerd B.},
pages = {325},
title = {{Environment, Development, and Evolution: Toward a Synthesis}},
year = {2003}
}
@book{Goldberg:1989:GAS:534133,
address = {Boston, MA, USA},
author = {Goldberg, David E},
edition = {1st},
isbn = {0201157675},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
title = {{Genetic Algorithms in Search, Optimization and Machine Learning}},
year = {1989}
}
@article{Yilmaz2014,
abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long short-term memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.0162v1},
author = {Yilmaz, Ozgur},
eprint = {arXiv:1410.0162v1},
journal = {arXiv preprint},
pages = {1--9},
title = {{Reservoir Computing using Cellular Automata}},
year = {2014}
}
@article{Tufte2005a,
abstract = {Today's reconfigurable technology provides vast parallelism that may be exploited in the design of a cellular computing machine (CCM). In this work a virtual Sblock FPGA is implemented on an existing FPGA, achieving not only an architecture in keeping with cellular computing principles but also suited to biologically inspired design methods. The design method proposed is a combination of evolution and development and results of running a developmental model on the CCM are presented.},
author = {Tufte, Gunnar and Haddow, Pauline C.},
doi = {10.1007/s11047-005-3665-8},
file = {:Users/oyvinrob/Downloads/Towards{\_}Development{\_}on{\_}a{\_}Silicon-based{\_}Cellular{\_}Co.pdf:pdf},
issn = {15677818},
journal = {Natural Computing},
keywords = {Artificial development,Cellular computing,Evolutionary design,Evolvable hardware},
number = {4},
pages = {387--416},
title = {{Towards development on a silicon-based cellular computing machine}},
volume = {4},
year = {2005}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Back1997,
abstract = {NK fitness landscapes are stochastically generated fitness functions on bit strings, parameterized (with N genes and K interactions between genes) so as to make them tunably rugged. Under the natural genetic operators of bit-flipping mutation or recombination, NK landscapes produce multiple domains of attrac- tion for the evolutionary dynamics. NK landscapes have been used in models of epistatic gene interactions, coevolution, genome growth, and Wrights shifting bal- ance model of adaptation. Theory for adaptive walks on NK landscapes has been derived, and generalizations that extend beyond Kauffmans original framework have been utilized in these applications.},
author = {B{\"{a}}ck, T and Fogel, D B and Michalewicz, Z},
doi = {10.1007/s00453-010-9472-3},
isbn = {0750303921},
issn = {01784617},
journal = {Evolutionary Computation},
pages = {1--11},
pmid = {4340592},
title = {{Handbook of Evolutionary Computation}},
volume = {2},
year = {1997}
}
@article{VonNeumann1993,
abstract = {The first draft of a report on the EDVAC written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine (ACE) as the definitive source for understanding the nature and design of a general-purpose digital computer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Von Neumann}, John and Godfrey, Michael D.},
doi = {10.1109/85.238389},
eprint = {arXiv:1011.1669v3},
isbn = {3540071148},
issn = {10586180},
journal = {IEEE Annals of the History of Computing},
number = {4},
pages = {27--75},
pmid = {15345047},
title = {{First Draft of a Report on the EDVAC}},
volume = {15},
year = {1993}
}
@article{Verstraeten2007,
abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
journal = {Neural Networks},
keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
month = {apr},
number = {3},
pages = {391--403},
title = {{An experimental unification of reservoir computing methods}},
volume = {20},
year = {2007}
}
@article{Jaeger2001,
abstract = {The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task.},
author = {Jaeger, Herbert},
doi = {citeulike-article-id:9635932},
isbn = {0-7803-9048-2},
issn = {18735223},
journal = {GMD Report},
pages = {1--47},
pmid = {19036266},
title = {{The "echo state" approach to analysing and training recurrent neural networks}},
volume = {148},
year = {2001}
}
@article{Snyder2013,
abstract = {This paper underscores the conjecture that intrinsic computation is maximal in systems at the “edge of chaos”. We study the relationship between dynamics and computational capability in random Boolean networks (RBN) for reservoir computing (RC). RC is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component (called the reservoir) that is perturbed by external input. The reservoir is often implemented as a homogeneous recurrent neural network, but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous. Random Boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir. A RBN is typically a closed system; to use it as a reservoir we extend it with an input layer. As a consequence of perturbation, the RBN does not necessarily fall into an attractor. Computational capability in RC arises from a tradeoff between separability and fading memory of inputs. We find the balance of these properties predictive of classification power and optimal at critical connectivity. These results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems, such as biomolecular substrates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.1744v1},
author = {Snyder, David and Goudarzi, Alireza and Teuscher, Christof},
doi = {10.1103/PhysRevE.87.042808},
eprint = {arXiv:1212.1744v1},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {4},
title = {{Computational capabilities of random automata networks for reservoir computing}},
volume = {87},
year = {2013}
}
@article{Gers2001,
abstract = {Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n).},
author = {Gers, F. A. and Schmidhuber, J.},
doi = {10.1109/72.963769},
isbn = {1045-9227 VO  - 12},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Context-free languages (CFLs),Context-sensitive languages (CSLs),Long short-term memory (LSTM),Recurrent neural networks (RNNs)},
number = {6},
pages = {1333--1340},
pmid = {18249962},
title = {{LSTM recurrent networks learn simple context-free and context-sensitive languages}},
volume = {12},
year = {2001}
}
@article{Sipper1999,
abstract = {The von Neumann architecture-which is based upon the principle of$\backslash$none complex processor that sequentially performs a single complex task$\backslash$nat a given moment-has dominated computing technology for the past 50$\backslash$nyears. Recently however, researchers have begun exploring alternative$\backslash$ncomputational systems based on entirely different principles. Although$\backslash$nemerging from disparate domains, the work behind these systems shares a$\backslash$ncommon computational philosophy, which the author calls cellular$\backslash$ncomputing. This philosophy promises to provide new means for doing$\backslash$ncomputation more efficiently-in terms of speed, cost, power dissipation,$\backslash$ninformation storage, and solution quality. Simultaneously, cellular$\backslash$ncomputing offers the potential of addressing much larger problem$\backslash$ninstances than previously possible, at least for some application$\backslash$ndomains. Cellular computing has attracted increasing research interest.$\backslash$nWork in this field has produced results that hold prospects for a bright$\backslash$nfuture. Yet questions must be answered before cellular computing can$\backslash$nbecome a mainstream paradigm. What classes of computational tasks are$\backslash$nmost suited to it? How do we match the specific properties and behaviors$\backslash$nof a given model to a suitable class of problems? At its heart, cellular$\backslash$ncomputing consists of three principles: simplicity, vast parallelism,$\backslash$nand locality},
author = {Sipper, Moshe},
doi = {10.1109/2.774914},
issn = {00189162},
journal = {Computer},
number = {7},
pages = {18--26},
title = {{Emergence of cellular computing}},
volume = {32},
year = {1999}
}
@techreport{Djupdal2003,
author = {Djupdal, Asbj{\o}rn},
file = {:Users/oyvinrob/Downloads/djupdal.pdf:pdf},
title = {{Konstruksjon av maskinvare for kj{\o}ring av sblokkbaserte eksperimenter}},
year = {2004}
}
@article{Markram2012,
author = {Markram, H. and Gerstner, W. and Sj{\"{o}}str{\"{o}}m, P. J.},
doi = {10.3389/fnsyn.2012.00002},
file = {:Users/oyvinrob/Downloads/fnsyn-04-00002.pdf:pdf},
issn = {16633563},
journal = {Frontiers in Synaptic Neuroscience},
number = {JULY},
pages = {2010--2012},
pmid = {22807913},
title = {{Spike-timing-dependent plasticity: A comprehensive overview}},
volume = {4},
year = {2012}
}
@book{Darwin1859,
abstract = {In The Origin of Species (1859) Darwin challenged many of the most deeply-held beliefs of the Western world. Arguing for a material, not divine, origin of species, he showed that new species are achieved by `natural selection'. Development, diversification, decay, extinction, and absence ofplan are all inherent to his theories. Darwin read prodigiously across many fields; he reflected on his experiences as a traveller; he experimented. His profoundly influential concept of `natural selection' condenses materials from past and present, from the Galapagos Islands to rural Staffordshire, from English back gardens tocolonial encounters. The Origin communicates the enthusiasm of original thinking in an open, descriptive style, and Darwin's emphasis on the value of diversity speaks more strongly now than ever. As well as a stimulating introduction and notes, this edition offers a register of the many writers referred to by Darwin in the text.},
author = {Darwin, Charles},
doi = {10.1038/475424a},
isbn = {9780882709192},
issn = {14764687},
pmid = {21796161},
title = {{On the Origin of Species}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3203181{\&}tool=pmcentrez{\&}rendertype=abstract},
year = {1859}
}
@article{Floreano2000,
abstract = {Evolutionary Robotics is a method for automatically generating artificial brains and morphologies of autonomous robots. This approach is useful both for investigating the design space of robotic applications and for testing scientific hypotheses of biological mechanisms and processes. In this chapter we provide an overview of methods and results of Evolutionary Robotics with robots of different shapes, dimensions, and operation features. We consider both simulated and physical robots with special consideration to the transfer between the two worlds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Floreano, Dario and Husbands, Phil and Nolfi, Stefano},
doi = {doi:10.1023/A:1021829228076},
eprint = {arXiv:1011.1669v3},
isbn = {0262640562},
issn = {1389-2576},
journal = {Evolutionary Robotics},
pages = {1423--1451},
title = {{Evolutionary robotics: The biology, intelligence, and technology of self-organizing machines}},
url = {http://en.scientificcommons.org/6958077},
year = {2000}
}
@misc{Spicher04atopological,
author = {Spicher, Antoine and Michel, Olivier and Giavitto, Jean-louis},
title = {{A Topological Framework for the Specification and the Simulation of Discrete Dynamical Systems}},
year = {2004}
}
@article{Hammer2002,
abstract = {... restrict recurrent connections to self-connections of the units such that backpropagation formulas can be ... Noisy time series prediction using a RNN and grammatical inference. ... RNNs: Design and Applications, chapter Comparison of Recurrent  Networks for Trajectory Generation. ...},
author = {Hammer, Barbara and Steil, Jochen J.},
isbn = {2930307021},
journal = {Proc. ESANN},
number = {April},
pages = {357--368},
title = {{Tutorial: Perspectives on learning with rnns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5565{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@techreport{Lundal2015a,
author = {Lundal, Per Thomas},
file = {:Users/oyvinrob/Downloads/lundal.pdf:pdf},
title = {{The Cellular Automata Research Platform: Revised, Rebuilt and Enhanced}},
year = {2015}
}
@inproceedings{Wu2016,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
isbn = {9781479999880},
issn = {15206149},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@book{Holland1975,
abstract = {Genetic algorithms were developed initially by Holland et al. in the 1960s and 1970s!!! Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and is Director of the University of Michigan/Santa Fe Institute Advanced Research Program.},
archivePrefix = {arXiv},
arxivId = {0262082136},
author = {Holland, J H},
booktitle = {Ann Arbor MI University of Michigan Press},
doi = {10.1137/1018105},
eprint = {0262082136},
isbn = {0262581116},
issn = {10834419},
pages = {183},
pmid = {15369078},
title = {{Adaptation in Natural and Artificial Systems}},
url = {http://www.citeulike.org/group/664/article/400721},
volume = {Ann Arbor},
year = {1975}
}
@article{Nichele2013,
author = {Nichele, S and Tufte, G},
file = {:Users/oyvinrob/Downloads/nichele{\_}ecal2013.pdf:pdf},
journal = {Ecal 2013},
pages = {63--70},
title = {{Evolution of Incremental Complex Behavior on Cellular Machines}},
year = {2013}
}
@techreport{Stovneng2014,
author = {St{\o}vneng, Ola Martin Tiseth},
file = {:Users/oyvinrob/Downloads/st{\o}vneng.pdf:pdf},
number = {May},
title = {{2D and 3D On-Chip Development of Cellular Automata Machines}},
year = {2014}
}
@techreport{Aamodt2005,
author = {Aamodt, Kjetil},
file = {:Users/oyvinrob/Downloads/Aamodt.pdf:pdf},
title = {{Kunstig utvikling: Utvidelse av FPGA-basert SBlock-plattform}},
year = {2005}
}
@article{Connelly1986,
abstract = {An analysis of game playing with a simplicity ordered tree of games.},
author = {Connelly, Robert and Berlekamp, Elwyn R. and Conway, John H. and Guy, Richard K.},
doi = {10.2307/2323620},
isbn = {1568811446},
issn = {00029890},
journal = {The American Mathematical Monthly},
number = {5},
pages = {411},
pmid = {16008188},
title = {{Winning Ways for Your Mathematical Plays.}},
url = {http://www.jstor.org/stable/2323620?origin=crossref},
volume = {93},
year = {1982}
}
