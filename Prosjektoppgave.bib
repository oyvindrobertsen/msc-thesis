Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Lawrence2001,
abstract = {Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarily, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. We introduce a new intelligent signal processing method which addresses the difficulties. The method proposed uses conversion into a symbolic representation with a self-organizing map, and grammatical inference with recurrent neural networks. We apply the method to the prediction of daily foreign exchange rates, addressing difficulties with non-stationarily, overfitting, and unequal a priori class probabilities, and we find significant predictability in comprehensive experiments covering 5 different foreign exchange rates. The method correctly predicts the direction of change for the next day with an error rate of 47.1{\%}. The error rate reduces to around 40{\%} when rejecting examples where the system has low confidence in its prediction. We show that the symbolic representation aids the extraction of symbolic knowledge from the trained recurrent neural networks in the form of deterministic finite state automata. These automata explain the operation of the system and are often relatively simple. Automata rules related to well known behavior such as trend following and mean reversal are extracted.},
author = {Lawrence, Steve and Giles, CL Lee and Fong, S and Lawrence, Steve and Tsoi, Ah Chung AC},
doi = {10.1023/A:1010884214864},
issn = {0885-6125},
journal = {Machine Learning},
number = {1},
pages = {161--183},
title = {{Noisy Time Series Prediction using Recurrent Neural Networks and Grammatical Inference}},
url = {http://link.springer.com/article/10.1023/A:1010884214864{\%}5Cnhttp://www.springerlink.com/index/X15G111873157W14.pdf{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=842255},
volume = {44},
year = {2001}
}
@misc{Bohte2002,
abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Bohte, Sander M. and Kok, Joost N. and {La Poutr{\'{e}}}, Han},
booktitle = {Neurocomputing},
doi = {10.1016/S0925-2312(01)00658-0},
isbn = {0925-2312},
issn = {09252312},
keywords = {Error-backpropagation,Spiking neurons,Temporal coding},
pages = {17--37},
pmid = {1000253838},
title = {{Error-backpropagation in temporally encoded networks of spiking neurons}},
volume = {48},
year = {2002}
}
@article{Fernando2003,
abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a {\&}8220;Liquid State Machine{\&}8221; that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass{\&}8217; Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this {\&}8220;for free{\&}8221;, and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
author = {Fernando, Chrisantha and Sojakka, Sampsa},
doi = {10.1007/978-3-540-39432-7_63},
isbn = {9783540200574},
issn = {03029743},
journal = {Advances in Artificial Life},
pages = {588--597},
title = {{Pattern Recognition in a Bucket}},
url = {http://www.springerlink.com/content/xlnymhf0qp946rce},
year = {2003}
}
@article{Lundal2015,
author = {Lundal, Per Thomas},
file = {:Users/oyvinrob/Downloads/lundal.pdf:pdf},
journal = {Master Thesis},
number = {June},
title = {{The Cellular Automata Research Platform: Revised, Rebuilt and Enhanced}},
year = {2015}
}
@article{Maass1997,
abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
author = {Maass, Wolfgang},
doi = {10.1016/S0893-6080(97)00011-7},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
number = {9},
pages = {1659--1671},
title = {{Networks of spiking neurons: The third generation of neural network models}},
volume = {10},
year = {1997}
}
@inproceedings{Steil2004,
abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08836v1},
author = {Steil, Jochen J.},
booktitle = {IEEE International Conference on Neural Networks - Conference Proceedings},
doi = {10.1109/IJCNN.2004.1380039},
eprint = {arXiv:1506.08836v1},
isbn = {0780383591},
issn = {10987576},
pages = {843--848},
title = {{Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity}},
volume = {2},
year = {2004}
}
@inproceedings{Tufte2009,
abstract = {Operation of developmental systems is in many ways similar to that of discrete dynamic networks. Applying such network analysis to developmental system enables investigation of the dynamic properties of development at different levels. In this work the basins of attraction of a developmental system is explored in order to gain information about the details from the interwoven nature of the development of structure and behaviour. The investigation show how such method of analysis can offer insight about the workings of developmental systems.},
author = {Tufte, Gunnar},
booktitle = {2009 IEEE Congress on Evolutionary Computation, CEC 2009},
doi = {10.1109/CEC.2009.4983215},
isbn = {9781424429592},
pages = {2209--2216},
title = {{The discrete dynamics of developmental systems}},
year = {2009}
}
@article{Schrauwen2007,
abstract = {Training recurrent neural networks is hard. Recently it has however been discovered that it is possible to just construct a r andom recurrent topology, and only train a single linear readout layer. State-of- the-art performance can easily be achieved with this setup, called R eservoir Computing. The idea can even be broadened by stating that any high di- mensional, driven dynamic system, operated in the correct dynamic r egime can be used as a temporal ‘kernel' which makes it possible to solve comp lex tasks using just linear post-processing techniques. This tutorial will give an overview of current research on theory, appl ica- tion and implementations of Reservoir Computing.},
author = {Schrauwen, Benjamin and Verstraeten, David and {Van Campenhout}, Jan},
doi = {1854/11063},
isbn = {2930307072},
journal = {Proceedings of the 15th European Symposium on Artificial Neural Networks},
number = {April},
pages = {471--82},
title = {{An overview of reservoir computing: theory, applications and implementations}},
year = {2007}
}
@article{Maass2002,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {11},
pages = {2531--2560},
pmid = {12433288},
title = {{Real-time computing without stable states: a new framework for neural computation based on perturbations.}},
volume = {14},
year = {2002}
}
@article{Tufte2016,
author = {Tufte, Gunnar and Lykkeb{\o}, Odd Rune},
file = {:Users/oyvinrob/Library/Application Support/Mendeley Desktop/Downloaded/Tufte, Lykkeb{\o} - 2016 - Evolution-in-Materio of a dynamical system with dynamical structures.pdf:pdf},
isbn = {9780262339360},
journal = {Proceedings of the Artificial Life Conference 2016},
title = {{Evolution-in-Materio of a dynamical system with dynamical structures}},
year = {2016}
}
@article{Langton1990,
abstract = {In order for computation to emerge spontaneously and become an important factor in the dynamics of a system, the material substrate must support the primitive functions required for computation: the transmission, storage, and modification of information. Under what conditions might we expect physical systems to support such computational primitives? This paper presents research on cellular automata which suggests that the optimal conditions for the support of information transmission, storage, and modification, are achieved in the vicinity of a phase transition. We observe surprising similarities between the behaviors of computations and systems near phase transitions, finding analogs of computational complexity classes and the halting problem within the phenomenology of phase transitions. We conclude that there is a fundamental connection between computation and phase transitions, especially second-order or "critical" transitions, and discuss some of the implications for our understanding of nature if such a connection is borne out. ?? 1990.},
archivePrefix = {arXiv},
arxivId = {adap-org/9306003},
author = {Langton, Chris G.},
doi = {10.1016/0167-2789(90)90064-V},
eprint = {9306003},
isbn = {0-7382-0232-0},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {12--37},
pmid = {3751941},
primaryClass = {adap-org},
title = {{Computation at the edge of chaos: Phase transitions and emergent computation}},
volume = {42},
year = {1990}
}
@book{Kumar2003,
editor = {Kumar, Sanjeev and Bentley, Peter},
isbn = {9780124287655},
publisher = {Elsevier},
title = {{On Growth, Form and Computers}},
year = {2003}
}
@article{Yilmaz2014,
abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long short-term memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.0162v1},
author = {Yilmaz, Ozgur},
eprint = {arXiv:1410.0162v1},
journal = {arXiv preprint},
pages = {1--9},
title = {{Reservoir Computing using Cellular Automata}},
year = {2014}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Verstraeten2007,
abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
journal = {Neural Networks},
keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
month = {apr},
number = {3},
pages = {391--403},
title = {{An experimental unification of reservoir computing methods}},
volume = {20},
year = {2007}
}
@article{Jaeger2001,
abstract = {The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task.},
author = {Jaeger, Herbert},
doi = {citeulike-article-id:9635932},
isbn = {0-7803-9048-2},
issn = {18735223},
journal = {GMD Report},
pages = {1--47},
pmid = {19036266},
title = {{The "echo state" approach to analysing and training recurrent neural networks}},
volume = {148},
year = {2001}
}
@book{Neumann:1966:TSA:1102024,
address = {Champaign, IL, USA},
author = {Neumann, John Von},
editor = {Burks, Arthur W},
publisher = {University of Illinois Press},
title = {{Theory of Self-Reproducing Automata}},
year = {1966}
}
@article{Snyder2013,
abstract = {This paper underscores the conjecture that intrinsic computation is maximal in systems at the “edge of chaos”. We study the relationship between dynamics and computational capability in random Boolean networks (RBN) for reservoir computing (RC). RC is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component (called the reservoir) that is perturbed by external input. The reservoir is often implemented as a homogeneous recurrent neural network, but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous. Random Boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir. A RBN is typically a closed system; to use it as a reservoir we extend it with an input layer. As a consequence of perturbation, the RBN does not necessarily fall into an attractor. Computational capability in RC arises from a tradeoff between separability and fading memory of inputs. We find the balance of these properties predictive of classification power and optimal at critical connectivity. These results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems, such as biomolecular substrates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.1744v1},
author = {Snyder, David and Goudarzi, Alireza and Teuscher, Christof},
doi = {10.1103/PhysRevE.87.042808},
eprint = {arXiv:1212.1744v1},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {4},
title = {{Computational capabilities of random automata networks for reservoir computing}},
volume = {87},
year = {2013}
}
@article{Miller2014,
abstract = {Evolution-in-materio (EIM) is the manipulation of a physical system by computer controlled evolution (CCE). It takes the position that to obtain useful functions from a physical system one needs to apply highly specific physical signals and place the system in a particular physical state. It argues that CCE is an effective methodology for doing this. One of the potential advantages of this is that artificial evolution can potentially exploit physical effects that are either too complex to understand or hitherto unknown. EIM is most commonly used as a methodology for implementing computation in physical systems. The method is a hybrid of analogue and classical computation in that it uses classical computers to program physical systems or analogue devices. Thus far EIM has only been attempted in a rather limited set of physical and chemical systems. This review paper examines past work related to EIM and discusses historical underpinnings behind such work. It describes latest developments, gives an analysis of the advantages and disadvantages of such work and the challenges that still remain.},
author = {Miller, Julian F and Harding, Simon L and Tufte, Gunnar},
doi = {10.1007/s12065-014-0106-6},
issn = {1864-5917},
journal = {Evolutionary Intelligence},
number = {1},
pages = {49--67},
title = {{Evolution-in-materio: evolving computation in materials}},
url = {http://dx.doi.org/10.1007/s12065-014-0106-6},
volume = {7},
year = {2014}
}
@article{Gers2001,
abstract = {Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n).},
author = {Gers, F. A. and Schmidhuber, J.},
doi = {10.1109/72.963769},
isbn = {1045-9227 VO  - 12},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Context-free languages (CFLs),Context-sensitive languages (CSLs),Long short-term memory (LSTM),Recurrent neural networks (RNNs)},
number = {6},
pages = {1333--1340},
pmid = {18249962},
title = {{LSTM recurrent networks learn simple context-free and context-sensitive languages}},
volume = {12},
year = {2001}
}
@article{Sipper1999,
abstract = {The von Neumann architecture-which is based upon the principle of$\backslash$none complex processor that sequentially performs a single complex task$\backslash$nat a given moment-has dominated computing technology for the past 50$\backslash$nyears. Recently however, researchers have begun exploring alternative$\backslash$ncomputational systems based on entirely different principles. Although$\backslash$nemerging from disparate domains, the work behind these systems shares a$\backslash$ncommon computational philosophy, which the author calls cellular$\backslash$ncomputing. This philosophy promises to provide new means for doing$\backslash$ncomputation more efficiently-in terms of speed, cost, power dissipation,$\backslash$ninformation storage, and solution quality. Simultaneously, cellular$\backslash$ncomputing offers the potential of addressing much larger problem$\backslash$ninstances than previously possible, at least for some application$\backslash$ndomains. Cellular computing has attracted increasing research interest.$\backslash$nWork in this field has produced results that hold prospects for a bright$\backslash$nfuture. Yet questions must be answered before cellular computing can$\backslash$nbecome a mainstream paradigm. What classes of computational tasks are$\backslash$nmost suited to it? How do we match the specific properties and behaviors$\backslash$nof a given model to a suitable class of problems? At its heart, cellular$\backslash$ncomputing consists of three principles: simplicity, vast parallelism,$\backslash$nand locality},
author = {Sipper, Moshe},
doi = {10.1109/2.774914},
issn = {00189162},
journal = {Computer},
number = {7},
pages = {18--26},
title = {{Emergence of cellular computing}},
volume = {32},
year = {1999}
}
@article{Markram2012,
author = {Markram, H. and Gerstner, W. and Sj{\"{o}}str{\"{o}}m, P. J.},
doi = {10.3389/fnsyn.2012.00002},
file = {:Users/oyvinrob/Downloads/fnsyn-04-00002.pdf:pdf},
issn = {16633563},
journal = {Frontiers in Synaptic Neuroscience},
number = {JULY},
pages = {2010--2012},
pmid = {22807913},
title = {{Spike-timing-dependent plasticity: A comprehensive overview}},
volume = {4},
year = {2012}
}
@inproceedings{Haddow2000,
abstract = {Can we realise the opportunities that lie in design by evolution$\backslash$nby using traditional technologies or are there better technologies which$\backslash$nwill allow us to fully realise the potential inherent in evolvable$\backslash$nhardware? The authors consider the characteristics of evolvable$\backslash$nhardware, especially for adaptive design, and discuss the demands that$\backslash$nthese characteristics place on the underlying technology. They suggest a$\backslash$npotential alternative to today's FPGA technology. The proposed$\backslash$narchitecture is particularly focused at reducing the genotype required$\backslash$nfor a given design by reducing the configuration data required for$\backslash$nunused routing resources and allowing partial configuration down to a$\backslash$nsingle CLB. In addition, to support adaptive hardware,$\backslash$nself-reconfiguration is enabled},
author = {Haddow, P.C. and Tufte, G.},
booktitle = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
doi = {10.1109/CEC.2000.870345},
isbn = {0-7803-6375-2},
pages = {553--560},
title = {{An evolvable hardware FPGA for adaptive hardware}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=870345},
volume = {1},
year = {2000}
}
@article{Hammer2002,
abstract = {... restrict recurrent connections to self-connections of the units such that backpropagation formulas can be ... Noisy time series prediction using a RNN and grammatical inference. ... RNNs: Design and Applications, chapter Comparison of Recurrent  Networks for Trajectory Generation. ...},
author = {Hammer, Barbara and Steil, Jochen J.},
isbn = {2930307021},
journal = {Proc. ESANN},
number = {April},
pages = {357--368},
title = {{Tutorial: Perspectives on learning with rnns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5565{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Tufte2005,
abstract = {Today's reconfigurable technology provides vast parallelism that may be exploited in the design of a cellular computing machine (CCM). In this work a virtual Sblock FPGA is implemented on an existing FPGA, achieving not only an architecture in keeping with cellular computing principles but also suited to biologically inspired design methods. The design method proposed is a combination of evolution and development and results of running a developmental model on the CCM are presented.},
author = {Tufte, Gunnar and Haddow, Pauline C.},
doi = {10.1007/s11047-005-3665-8},
issn = {15677818},
journal = {Natural Computing},
keywords = {Artificial development,Cellular computing,Evolutionary design,Evolvable hardware},
number = {4},
pages = {387--416},
title = {{Towards development on a silicon-based cellular computing machine}},
volume = {4},
year = {2005}
}
@inproceedings{Wu2016,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
isbn = {9781479999880},
issn = {15206149},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{Pipa2010,
author = {Pipa, Gordon and Cao, Robin},
file = {:Users/oyvinrob/Library/Application Support/Mendeley Desktop/Downloaded/Pipa, Cao - 2010 - Extended Liquid Computing in Networks of Spiking Neurons Supervisor Spiking Neurons.pdf:pdf},
title = {{Extended Liquid Computing in Networks of Spiking Neurons Supervisor Spiking Neurons}},
year = {2010}
}
@article{Mitchell1994,
abstract = {We present results from experiments in which a genetic algorithm (GA) was used to evolve cellular automata (CAs) to perform a particular computational task - one-dimensional density classification. We look in detail at the evolutionary mechanisms producing the GA's behavior on this task and the impediments faced by the GA. In particular, we identify four "epochs of innovation" in which new CA strategies for solving the problem are discovered by the GA, describe how these strategies are implemented in CA rule tables, and identify the GA mechanisms underlying their discovery. The epochs are characterized by a breaking of the task's symmetries on the part of the GA. The symmetry breaking results in a short-term fitness gain but ultimately prevents the discovery of the most highly fit strategies. We discuss the extent to which symmetry breaking and other impediments are general phenomena in any GA search. ?? 1994.},
author = {Mitchell, Melanie and Crutchfield, James P. and Hraber, Peter T.},
doi = {10.1016/0167-2789(94)90293-3},
isbn = {0167-2789},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {361--391},
title = {{Evolving cellular automata to perform computations: mechanisms and impediments}},
volume = {75},
year = {1994}
}
@book{Wolfram2003,
abstract = {These original papers on cellular automata and complexity, some of which are widely known in the scientific community, provide a highly readable account of what has become a major new field of science, with important implications for physics, biology, economics, computer science, and many other areas.},
author = {Wolfram, Stephen},
booktitle = {Computers {\&} Mathematics with Applications},
doi = {10.1016/S0898-1221(03)80147-5},
isbn = {0201626640},
issn = {08981221},
keywords = {Cellular automata,complexity},
number = {10-11},
pages = {1779},
title = {{Cellular automata and complexity: Collected papers}},
volume = {45},
year = {2003}
}
@book{Holland1975,
abstract = {Genetic algorithms were developed initially by Holland et al. in the 1960s and 1970s!!! Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and is Director of the University of Michigan/Santa Fe Institute Advanced Research Program.},
archivePrefix = {arXiv},
arxivId = {0262082136},
author = {Holland, J H},
booktitle = {Ann Arbor MI University of Michigan Press},
doi = {10.1137/1018105},
eprint = {0262082136},
isbn = {0262581116},
issn = {10834419},
pages = {183},
pmid = {15369078},
title = {{Adaptation in Natural and Artificial Systems}},
url = {http://www.citeulike.org/group/664/article/400721},
volume = {Ann Arbor},
year = {1975}
}
@incollection{ECTufte2009,
author = {Tufte, Gunnar},
booktitle = {Evolutionary Computation},
chapter = {12},
file = {:Users/oyvinrob/Library/Application Support/Mendeley Desktop/Downloaded/Tufte - 2009 - From Evo to EvoDevo Mapping and Adaptation in Artificial Development.pdf:pdf},
isbn = {9789537619343},
title = {{From Evo to EvoDevo: Mapping and Adaptation in Artificial Development}},
year = {2009}
}
